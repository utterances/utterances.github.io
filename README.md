## Interaction Design Research
#### Multi-touch Visual Environment for Mobile
To examine visual feedback in multi-touch interaction design, I built a visual programming environment for multi-touch tablet for assembling interfaces. This serves as a platform to explore visual feedback in multi-touch interactions. Multiple visual feedback paradigms are implemented on top of a common core visual vocabulary, consisted of visual entities such as regions and links and containers.
![tapperware overview](http://www-personal.umich.edu/~yangqi/portfolio/images/multi-overview.jpg)

This environment can be used to rapidly construct musical instruments, sample tasks were designed and used in human-subject usability study. In these example, a musical keyboard and multi-touch mixer instrument is built.
[![task video](http://www-personal.umich.edu/~yangqi/portfolio/images/multi-example.jpg)](https://vimeo.com/116926224)
[Video: task 1](https://vimeo.com/116926224)
[Video: task 2](https://vimeo.com/116926223)

##### Tools
[urMus](http://urmus.eecs.umich.edu), a cross-platform audio and visual interaction environment, Lua, iOS.

##### Publications pending

#### Gesture-augmented Keyboard Instrument
Using Kinect Depth sensor to augment traditional keyboard instrument with a 3D gesture space, and top-down projection is used for visual feedback at the site of the gesture interaction.
[![keyboard setup](http://www-personal.umich.edu/~yangqi/portfolio/images/key-overview.jpg)](https://vimeo.com/44947845)
[Video demo](https://vimeo.com/44947845)

This novel interaction model enables us to explore different visualizations:
![keyboard visualization](http://www-personal.umich.edu/~yangqi/portfolio/images/key-vis.jpg)

##### Publication
*Evaluating Gesture-Augmented Piano Performance*, Qi Yang, Georg Essl 	
[CMJ 2014](http://www.mitpressjournals.org/doi/abs/10.1162/COMJ_a_00277)
[PDF](http://www-personal.umich.edu/~yangqi/pubs/CMJ 2014 Yang.pdf)

*Visual Associations in Augmented Keyboard Performance*, Qi Yang, Georg Essl 	
[NIME 2013](http://www-personal.umich.edu/~yangqi/yangqi.png)

*Augmented Piano Performance using a Depth Camera*, Qi Yang, Georg Essl 	
[NIME 2012](http://www-personal.umich.edu/~yangqi/yangqi.png)

## Design & Data
#### Mobile Sensor Data Analysis and Visualization
As part of the ExFlu study by University of Michigan School of Public Health, I cleaned and analyzed multi-sensory data collected from 100 phones over 3 month. These include bluetooth and wifi contacts, accelerometer, and battery.

Between-phone Bluetooth contact data are used to visualize social contact between study participants.
_Contact network between 6 dormitory, by hour of day:_
![hourly visualization](http://www-personal.umich.edu/~yangqi/portfolio/images/iepi-hourly.jpg)

_Contact network between 6 dormitory, :_
[![all day visualization](http://www-personal.umich.edu/~yangqi/portfolio/images/iepi-all-thumb.jpg)](http://www-personal.umich.edu/~yangqi/portfolio/images/iepi-all.jpg)

I also coordinated the collection of GPS position of local wifi access points. These data enabled me to visualize on campus 

#### Localization using smart phone wifi access point data


## Web Development
Harvest Mission Community Church
